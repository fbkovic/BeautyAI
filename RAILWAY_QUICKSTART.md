# Railway Quick Start - Ollama Deployment

## Schritt 1: Railway Account erstellen

1. Gehen Sie zu **https://railway.app**
2. Klicken Sie auf **"Start a New Project"**
3. WÃ¤hlen Sie **"Login with GitHub"** (empfohlen)
4. Autorisiere Railway, auf Ihr GitHub-Repository zuzugreifen

## Schritt 2: Neues Projekt erstellen

1. Im Railway Dashboard klicken Sie auf **"New Project"**
2. WÃ¤hlen Sie **"Deploy from GitHub repo"**
3. WÃ¤hlen Sie Ihr Repository: **`BiGKerem/BeautyAI`**
4. Railway erstellt automatisch ein neues Projekt

## Schritt 3: Ollama Service hinzufÃ¼gen

1. Im Projekt-Dashboard klicken Sie auf **"+ New"** â†’ **"GitHub Repo"**
2. WÃ¤hlen Sie erneut Ihr Repository
3. Railway erkennt automatisch das `Dockerfile.ollama`
4. Falls nicht:
   - Klicken Sie auf **"Settings"** â†’ **"Source"**
   - WÃ¤hlen Sie **"Dockerfile"** als Build Type
   - Setzen Sie **Dockerfile Path** auf: `Dockerfile.ollama`

## Schritt 4: Domain generieren

1. Klicken Sie auf den Service **"ollama"** (oder den generierten Namen)
2. Gehen Sie zu **"Settings"** â†’ **"Networking"**
3. Klicken Sie auf **"Generate Domain"**
4. Kopieren Sie die generierte URL (z.B. `ollama-production.up.railway.app`)

## Schritt 5: Modell herunterladen

### Option A: Ãœber Railway Shell (Empfohlen)

1. Im Service-Dashboard klicken Sie auf **"View Logs"**
2. Klicken Sie auf **"Open Shell"** (oben rechts)
3. FÃ¼hren Sie aus:
   ```bash
   ollama pull llama3.2
   ```
4. Warten Sie, bis der Download abgeschlossen ist (kann einige Minuten dauern)

### Option B: Ãœber API

```bash
curl -X POST https://your-railway-url.railway.app/api/pull \
  -H "Content-Type: application/json" \
  -d '{"name": "llama3.2"}'
```

## Schritt 6: Testen

```bash
# PrÃ¼fen Sie, ob Ollama lÃ¤uft
curl https://your-railway-url.railway.app/api/tags

# Sollte eine JSON-Antwort mit verfÃ¼gbaren Modellen zurÃ¼ckgeben
```

## Schritt 7: Vercel Environment Variable setzen

1. Gehen Sie zu **Vercel Dashboard** â†’ **Ihr Projekt** â†’ **Settings**
2. Klicken Sie auf **"Environment Variables"**
3. FÃ¼gen Sie eine neue Variable hinzu:
   - **Name:** `OLLAMA_BASE_URL`
   - **Value:** `https://your-railway-url.railway.app`
   - **Environment:** Production, Preview, Development (alle)
4. Klicken Sie auf **"Save"**
5. **Wichtig:** Deployen Sie die App erneut, damit die Variable aktiv wird

## Schritt 8: Vercel App neu deployen

```bash
# Oder Ã¼ber Vercel Dashboard
# Settings â†’ Deployments â†’ Redeploy
```

Oder Ã¼ber CLI:
```bash
vercel --prod
```

## Fertig! ðŸŽ‰

Ihr AI Assistant sollte jetzt funktionieren!

### Testen Sie es:

1. Ã–ffnen Sie Ihre Vercel-App
2. Gehen Sie zu **"ðŸ¤– AI Assistant"**
3. Stellen Sie eine Frage, z.B.: "Wie viele Kunden haben wir?"

## Troubleshooting

### Ollama startet nicht
- PrÃ¼fen Sie die Logs in Railway: Service â†’ View Logs
- Stellen Sie sicher, dass Port 11434 exponiert ist

### Modell nicht verfÃ¼gbar
- PrÃ¼fen Sie die Logs: `ollama list` in Railway Shell
- Laden Sie das Modell erneut: `ollama pull llama3.2`

### Timeout-Fehler
- PrÃ¼fen Sie die Railway-URL in Vercel Environment Variables
- Stellen Sie sicher, dass die URL mit `https://` beginnt

### Kosten

Railway Free Tier:
- $5 kostenloses Guthaben pro Monat
- Genug fÃ¼r Ollama mit llama3.2 (kleines Modell)
- FÃ¼r grÃ¶ÃŸere Modelle: Pro Plan ($20/Monat) oder Pay-as-you-go

## Alternative: Render

Falls Railway nicht funktioniert:

1. Gehen Sie zu **https://render.com**
2. **New** â†’ **Web Service**
3. **Docker Image:** `ollama/ollama:latest`
4. **Start Command:** `ollama serve`
5. Kopieren Sie die URL und setzen Sie `OLLAMA_BASE_URL` in Vercel

