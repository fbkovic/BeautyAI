#!/bin/bash

echo "ğŸ¤– Ollama Installation fÃ¼r AI Assistant"
echo "========================================"

# PrÃ¼fe ob Ollama bereits installiert ist
if command -v ollama &> /dev/null; then
    echo "âœ… Ollama ist bereits installiert"
    ollama --version
else
    echo "ğŸ“¥ Installiere Ollama..."
    
    # macOS
    if [[ "$OSTYPE" == "darwin"* ]]; then
        if command -v brew &> /dev/null; then
            echo "Installiere mit Homebrew..."
            brew install ollama
        else
            echo "âŒ Homebrew nicht gefunden. Bitte installiere Ollama manuell:"
            echo "   https://ollama.ai/download"
            exit 1
        fi
    # Linux
    elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
        echo "Installiere mit curl..."
        curl -fsSL https://ollama.ai/install.sh | sh
    # Windows
    else
        echo "âŒ Bitte installiere Ollama manuell fÃ¼r Windows:"
        echo "   https://ollama.ai/download"
        exit 1
    fi
fi

echo ""
echo "ğŸš€ Starte Ollama Server..."
# Starte Ollama im Hintergrund
ollama serve &
OLLAMA_PID=$!

# Warte bis Ollama bereit ist
echo "â³ Warte auf Ollama Server..."
sleep 3

# PrÃ¼fe ob Ollama lÃ¤uft
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "âœ… Ollama Server lÃ¤uft!"
else
    echo "âš ï¸  Ollama Server startet noch..."
    sleep 2
fi

echo ""
echo "ğŸ“¥ Lade Modell llama3.2 herunter..."
echo "   (Das kann einige Minuten dauern, je nach Internetgeschwindigkeit)"
ollama pull llama3.2

if [ $? -eq 0 ]; then
    echo ""
    echo "âœ… Installation abgeschlossen!"
    echo ""
    echo "VerfÃ¼gbare Modelle:"
    ollama list
    echo ""
    echo "ğŸ’¡ Tipp: Lass 'ollama serve' im Hintergrund laufen oder starte es manuell:"
    echo "   ollama serve"
else
    echo "âŒ Fehler beim Herunterladen des Modells"
    exit 1
fi

