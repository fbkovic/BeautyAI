# Render.com Deployment - Schritt f√ºr Schritt

## Schritt 1: Render Account erstellen

1. Gehen Sie zu **https://render.com**
2. Klicken Sie auf **"Get Started for Free"** (oben rechts)
3. W√§hlen Sie **"Sign up with GitHub"** (empfohlen)
4. Autorisiere Render, auf Ihr GitHub-Repository zuzugreifen
5. ‚úÖ **Fertig!** Sie sind jetzt eingeloggt

---

## Schritt 2: Neues Web Service erstellen

1. Im Render Dashboard klicken Sie auf **"+ New"** (oben rechts)
2. W√§hlen Sie **"Web Service"**
3. Sie sehen jetzt eine Liste Ihrer GitHub-Repositories

---

## Schritt 3: Repository ausw√§hlen

1. Suchen Sie nach **"BeautyAI"** oder **"BiGKerem/BeautyAI"**
2. Klicken Sie auf **"Connect"** neben Ihrem Repository
3. Falls das Repository nicht erscheint:
   - Klicken Sie auf **"Configure account"**
   - W√§hlen Sie alle Repositories oder nur "BeautyAI"
   - Klicken Sie auf **"Save"**

---

## Schritt 4: Service konfigurieren

F√ºllen Sie folgende Felder aus:

### Basis-Informationen:
- **Name:** `ollama-service` (oder ein anderer Name)
- **Region:** W√§hlen Sie die n√§chstgelegene Region (z.B. `Frankfurt` f√ºr Deutschland)

### Build & Deploy:
- **Branch:** `main` (oder `master`)
- **Root Directory:** (leer lassen)
- **Environment:** `Docker`
- **Dockerfile Path:** `Dockerfile.ollama`
- **Docker Build Context:** `.` (Punkt)

### Plan:
- **Plan:** W√§hlen Sie **"Free"** (kostenlos!)

### Start Command:
- **Start Command:** `ollama serve`

### Environment Variables:
Klicken Sie auf **"Add Environment Variable"**:
- **Key:** `OLLAMA_HOST`
- **Value:** `0.0.0.0`
- Klicken Sie auf **"Add"**

---

## Schritt 5: Service erstellen

1. Scrollen Sie nach unten
2. Klicken Sie auf **"Create Web Service"**
3. ‚è≥ **Warten Sie** - Render baut jetzt Ihr Docker-Image (5-10 Minuten)

---

## Schritt 6: Deployment √ºberwachen

1. Sie sehen jetzt die **Build-Logs**
2. Warten Sie, bis Sie sehen:
   ```
   Build successful
   Your service is live at https://ollama-service.onrender.com
   ```
3. ‚úÖ **Fertig!** Notieren Sie sich die URL

---

## Schritt 7: Modell herunterladen

### Option A: √úber Render Shell (Empfohlen)

1. Im Service-Dashboard klicken Sie auf **"Shell"** (oben rechts)
2. Ein Terminal √∂ffnet sich
3. F√ºhren Sie aus:
   ```bash
   ollama pull llama3.2
   ```
4. ‚è≥ **Warten Sie** - Der Download kann 5-10 Minuten dauern
5. Sie sehen: `pulling manifest...`, `downloading...`, `success`

### Option B: √úber API (Alternative)

√ñffnen Sie ein Terminal auf Ihrem Computer:
```bash
curl -X POST https://your-service-name.onrender.com/api/pull \
  -H "Content-Type: application/json" \
  -d '{"name": "llama3.2"}'
```

---

## Schritt 8: Testen

Pr√ºfen Sie, ob Ollama l√§uft:

```bash
curl https://your-service-name.onrender.com/api/tags
```

Sie sollten eine JSON-Antwort mit verf√ºgbaren Modellen sehen.

---

## Schritt 9: Vercel konfigurieren

1. Gehen Sie zu **Vercel Dashboard**: https://vercel.com/dashboard
2. W√§hlen Sie Ihr Projekt **"beauty-crm"**
3. Klicken Sie auf **"Settings"** (oben)
4. Klicken Sie auf **"Environment Variables"** (links)
5. Klicken Sie auf **"Add New"**
6. F√ºllen Sie aus:
   - **Key:** `OLLAMA_BASE_URL`
   - **Value:** `https://your-service-name.onrender.com` (Ihre Render-URL)
   - **Environment:** W√§hlen Sie alle aus (Production, Preview, Development)
7. Klicken Sie auf **"Save"**

---

## Schritt 10: Vercel App neu deployen

### Option A: √úber Vercel Dashboard

1. Gehen Sie zu **Deployments** (links)
2. Klicken Sie auf die drei Punkte (‚ãØ) neben dem letzten Deployment
3. W√§hlen Sie **"Redeploy"**
4. Best√§tigen Sie

### Option B: √úber Terminal

```bash
vercel --prod
```

---

## Schritt 11: Testen Sie den AI Assistant

1. √ñffnen Sie Ihre Vercel-App
2. Gehen Sie zu **"ü§ñ AI Assistant"**
3. Stellen Sie eine Frage, z.B.: "Wie viele Kunden haben wir?"
4. ‚úÖ **Fertig!** Der AI Assistant sollte jetzt funktionieren!

---

## ‚ö†Ô∏è Wichtige Hinweise

### Render Free Tier Limits:
- ‚è∞ **15 Minuten Inaktivit√§t** ‚Üí Service schl√§ft ein
- üöÄ **Erste Anfrage** kann 30-60 Sekunden dauern (Cold Start)
- üíæ **512 MB RAM** verf√ºgbar
- üìä **100 GB Bandbreite** pro Monat

### Tipps:
- F√ºr Production: Upgrade auf **Starter Plan** ($7/Monat) empfohlen
- Service bleibt dann immer wach
- Keine Cold Starts

---

## üÜò Troubleshooting

### Service startet nicht:
- Pr√ºfen Sie die **Logs** in Render Dashboard
- Stellen Sie sicher, dass `Dockerfile.ollama` existiert
- Pr√ºfen Sie den **Start Command**: `ollama serve`

### Modell nicht verf√ºgbar:
- √ñffnen Sie die **Shell** in Render
- F√ºhren Sie aus: `ollama list`
- Falls leer: `ollama pull llama3.2`

### Timeout-Fehler:
- Pr√ºfen Sie die Render-URL in Vercel Environment Variables
- Stellen Sie sicher, dass die URL mit `https://` beginnt
- Warten Sie nach dem ersten Start 1-2 Minuten (Cold Start)

### Service schl√§ft ein:
- Das ist normal im Free Tier
- Erste Anfrage nach 15+ Minuten Inaktivit√§t dauert l√§nger
- L√∂sung: Upgrade auf Starter Plan oder regelm√§√üig pingen

---

## ‚úÖ Checkliste

- [ ] Render Account erstellt
- [ ] Web Service erstellt
- [ ] Dockerfile.ollama konfiguriert
- [ ] Service deployed
- [ ] Modell (llama3.2) heruntergeladen
- [ ] Ollama getestet (curl /api/tags)
- [ ] OLLAMA_BASE_URL in Vercel gesetzt
- [ ] Vercel App neu deployed
- [ ] AI Assistant getestet

---

**Viel Erfolg! üöÄ**

